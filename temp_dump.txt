from __future__ import annotations

import json
import logging
import os
import re
from pathlib import Path
from typing import Any, Dict, List

from backend.core.logic.report_analysis.report_parsing import (
    build_block_fuzzy,
    detect_bureau_order,
)
from backend.core.logic.report_analysis.text_provider import load_cached_text
from backend.core.text.text_provider import load_text_with_layout
from backend.core.logic.report_analysis.column_reader import extract_bureau_table
from backend.core.logic.report_analysis.block_segmenter import segment_account_blocks
from backend.core.logic.utils.text_parsing import extract_account_blocks

# Optional G1 infra (label/bureau detection)
try:  # pragma: no cover - optional import
    from ._bureau_parse_utils import (
        find_label_groups as G1_find_label_groups,
        is_bureau as G1_is_bureau,
        norm as G1_norm,
    )
    _G1_AVAILABLE = True
except Exception:  # pragma: no cover - best effort
    _G1_AVAILABLE = False


def load_account_blocks(session_id: str) -> List[Dict[str, Any]]:
    """Load previously exported account blocks for ``session_id``.

    The blocks are expected under ``traces/blocks/<session_id>/_index.json``.
    The index must be a JSON array where each element is a mapping with
    exactly the keys ``{"i", "heading", "file"}``. If the directory or index
    file is missing, an entry is malformed, or a referenced block file cannot
    be read/parsed, the function fails softly and simply returns an empty
    list.

    Parameters
    ----------
    session_id:
        Identifier used for locating ``traces/blocks/<session_id>``.

    Returns
    -------
    list[dict]
        List of block dictionaries of the form ``{"heading": str,
        "lines": list[str]}``.
    """

    base = Path("traces") / "blocks" / session_id
    index_path = base / "_index.json"
    if not index_path.exists():
        return []
    try:
        idx = json.loads(index_path.read_text(encoding="utf-8"))
    except Exception:
        return []

    blocks: List[Dict[str, Any]] = []
    expected_keys = {"i", "heading", "file"}
    for entry in idx or []:
        if not isinstance(entry, dict):
            continue
        if set(entry.keys()) != expected_keys:
            # ignore unexpected/legacy index rows
            continue
        f = entry.get("file")
        if not isinstance(f, str) or not f:
            continue
        try:
            data = json.loads(Path(f).read_text(encoding="utf-8"))
            if isinstance(data, dict) and "heading" in data and "lines" in data:
                blocks.append(data)
        except Exception:
            continue
    return blocks


logger = logging.getLogger(__name__)


ENRICH_ENABLED = os.getenv("BLOCK_ENRICH", "1") != "0"
BLOCK_DEBUG = os.getenv("BLOCK_DEBUG", "0") == "1"
USE_LAYOUT_TEXT = os.getenv("USE_LAYOUT_TEXT", "1") != "0"


FIELD_LABELS: dict[str, str] = {
    "account #": "account_number_display",
    "high balance": "high_balance",
    "last verified": "last_verified",
    "date of last activity": "date_of_last_activity",
    "date reported": "date_reported",
    "date opened": "date_opened",
    "balance owed": "balance_owed",
    "closed date": "closed_date",
    "account rating": "account_rating",
    "account description": "account_description",
    "dispute status": "dispute_status",
    "creditor type": "creditor_type",
    "account status": "account_status",
    "payment status": "payment_status",
    "creditor remarks": "creditor_remarks",
    "payment amount": "payment_amount",
    "last payment": "last_payment",
    "term length": "term_length",
    "past due amount": "past_due_amount",
    "account type": "account_type",
    "payment frequency": "payment_frequency",
    "credit limit": "credit_limit",
}

# Regex label mapping for robust SmartCredit variants (prefix match)
LABEL_MAP: list[tuple[re.Pattern[str], str]] = [
    (re.compile(r"^ACCOUNT\s*#", re.I), "account_number_display"),
    (re.compile(r"^HIGH\s*BALANCE", re.I), "high_balance"),
    (re.compile(r"^LAST\s*VERIFIED", re.I), "last_verified"),
    (re.compile(r"^DATE\s+OF\s+LAST\s+ACTIVITY", re.I), "date_of_last_activity"),
    (re.compile(r"^DATE\s+REPORTED", re.I), "date_reported"),
    (re.compile(r"^DATE\s+OPENED", re.I), "date_opened"),
    (re.compile(r"^BALANCE\s+OWED", re.I), "balance_owed"),
    (re.compile(r"^CLOSED\s+DATE", re.I), "closed_date"),
    (re.compile(r"^ACCOUNT\s+RATING", re.I), "account_rating"),
    (re.compile(r"^ACCOUNT\s+DESCRIPTION", re.I), "account_description"),
    (re.compile(r"^DISPUTE\s+STATUS", re.I), "dispute_status"),
    (re.compile(r"^CREDITOR\s+TYPE", re.I), "creditor_type"),
    (re.compile(r"^ACCOUNT\s+STATUS", re.I), "account_status"),
    (re.compile(r"^PAYMENT\s+STATUS", re.I), "payment_status"),
    (re.compile(r"^CREDITOR\s+REMARKS", re.I), "creditor_remarks"),
    (re.compile(r"^PAYMENT\s+AMOUNT", re.I), "payment_amount"),
    (re.compile(r"^LAST\s+PAYMENT", re.I), "last_payment"),
    (re.compile(r"^TERM\s+LENGTH", re.I), "term_length"),
    (re.compile(r"^PAST\s+DUE\s+AMOUNT", re.I), "past_due_amount"),
    (re.compile(r"^ACCOUNT\s+TYPE", re.I), "account_type"),
    (re.compile(r"^PAYMENT\s+FREQUENCY", re.I), "payment_frequency"),
    (re.compile(r"^CREDIT\s+LIMIT", re.I), "credit_limit"),
    # Optional sections
    (re.compile(r"^TWO[- ]YEAR\s+PAYMENT\s+HISTORY", re.I), "two_year_payment_history"),
    (re.compile(r"^DAYS\s+LATE\s*[-–]\s*7\s*YEAR\s*HISTORY", re.I), "seven_year_days_late"),
]


def _norm_hdr(s: str) -> str:
    s = (s or "").replace("\u00A0", " ").replace("®", " ").replace("™", " ")
    s = re.sub(r"\s+", " ", s.strip())
    s = re.sub(r"[^A-Za-z ]+", " ", s)
    return s.upper()


def _find_bureau_header_idx(lines: list[str]) -> tuple[int | None, list[str] | None]:
    """Return (index, order) for the first tri-bureau header occurrence.

    Supports headers split across two lines.
    """
    tokens = ["TRANSUNION", "EXPERIAN", "EQUIFAX"]
    for i in range(len(lines)):
        joined = _norm_hdr(lines[i])
        pairs = [joined]
        if i + 1 < len(lines):
            pairs.append(_norm_hdr(lines[i] + " " + lines[i + 1]))
        for text in pairs:
            if all(t in text for t in tokens):
                positions = {t: text.find(t) for t in tokens}
                order = [k.lower() for k, _ in sorted(positions.items(), key=lambda x: x[1])]
                return i, order
    return None, None


# Labels grouped per SmartCredit layout
LABELS_TOP = [
    "Account #",
    "High Balance",
    "Last Verified",
    "Date of Last Activity",
    "Date Reported",
    "Date Opened",
    "Balance Owed",
    "Closed Date",
    "Account Rating",
    "Account Description",
    "Dispute Status",
    "Creditor Type",
]

LABELS_BOTTOM = [
    "Account Status",
    "Payment Status",
    "Creditor Remarks",
    "Payment Amount",
    "Last Payment",
    "Term Length",
    "Past Due Amount",
    "Account Type",
    "Payment Frequency",
    "Credit Limit",
]


def _norm_token(s: str) -> str:
    s = (s or "").replace("\u00A0", " ").replace("®", " ").replace("™", " ")
    s = re.sub(r"\s+", " ", s.strip())
    s = s.rstrip(":")
    return s.upper()


def _is_noise_line(s: str) -> bool:
    t = (s or "").strip()
    return t in {"", "®", "--", "-"}


def _find_label_block(lines: list[str], labels: list[str]) -> int | None:
    """Find the index where a label block starts by matching the first label."""
    first = _norm_token(labels[0])
    for i, ln in enumerate(lines):
        if _norm_token(ln).startswith(first):
            return i
    return None


def _collect_values_sequence(lines: list[str], start_idx: int, stop_tokens: set[str], limit: int | None = None) -> list[str]:
    vals: list[str] = []
    i = max(0, start_idx)
    while i < len(lines):
        tok = _norm_token(lines[i])
        if tok in stop_tokens:
            break
        if _is_noise_line(lines[i]):
            i += 1
            continue
        vals.append(lines[i].strip())
        if limit and len(vals) >= limit:
            break
        i += 1
    return vals


def _split_equal_parts(seq: list[str], parts: int) -> list[list[str]] | None:
    if parts <= 0:
        return None
    if len(seq) % parts != 0:
        return None
    step = len(seq) // parts
    return [seq[i * step : (i + 1) * step] for i in range(parts)]


def _zip_values_to_labels(raw_values: list[str], labels: list[str]) -> dict[str, str]:
    """Map values to labels with simple continuation joining and padding.

    - If extra values exist, join them to the last label's value.
    - Clean "--" or "-" to empty.
    """
    cleaned = [(v or "").strip() for v in raw_values]
    # Pad
    if len(cleaned) < len(labels):
        cleaned = cleaned + [""] * (len(labels) - len(cleaned))
    # Join surplus
    if len(cleaned) > len(labels):
        head = cleaned[: len(labels)]
        tail = cleaned[len(labels) :]
        joiner = " ".join(x for x in tail if x)
        if joiner:
            head[-1] = (head[-1] + " " + joiner).strip() if head[-1] else joiner
        cleaned = head
    out: dict[str, str] = {}
    for lab, val in zip(labels, cleaned):
        v = val
        if v in {"--", "-"}:
            v = ""
        out[lab] = v
    return out


def _segments_by_bureau(block_lines: list[str], order: list[str]) -> dict[str, dict[str, list[str]]] | None:
    """Return values per bureau for top/bottom sections when OCR yields stacked segments.

    Returns { 'top': {bureau: [..]}, 'bottom': {bureau: [..]} } or None if not matched.
    """
    if not order:
        return None
    # Normalize tokens and prepare stop set
    tokens = {"TRANSUNION", "EXPERIAN", "EQUIFAX"}
    stop_tokens = tokens.union({_norm_token(t) for t in LABELS_TOP + LABELS_BOTTOM})

    # Find the first occurrence of each bureau in order
    idxs: dict[str, int] = {}
    for i, ln in enumerate(block_lines):
        tok = _norm_token(ln)
        if tok in tokens and tok.lower() in order and tok.lower() not in idxs:
            idxs[tok.lower()] = i
            if len(idxs) == 3:
                break
    if len(idxs) < 2:  # need at least 2 to segment
        return None

    # Attempt to locate top/bottom label anchors
    top_anchor = _find_label_block(block_lines, LABELS_TOP)
    bot_anchor = _find_label_block(block_lines, LABELS_BOTTOM)

    result: dict[str, dict[str, list[str]]] = {"top": {}, "bottom": {}}

    # Collect TOP values if anchor exists and bureau tokens follow
    if top_anchor is not None:
        # For each bureau, collect up to len(LABELS_TOP) values after its token
        for b in order:
            pos = idxs.get(b)
            if pos is None:
                result["top"][b] = []
                continue
            # Stop at next bureau token or bottom anchor if present
            next_positions = [idxs.get(ob) for ob in order if idxs.get(ob) and idxs.get(ob) > pos]
            bound = min([p for p in next_positions if p is not None] + ([bot_anchor] if bot_anchor is not None else []) + [len(block_lines)])
            vals = _collect_values_sequence(block_lines, pos + 1, tokens.union({_norm_token("Two-Year Payment History"), _norm_token("Days Late - 7 Year History")}), limit=None)
            # Trim to within bound window
            # Since _collect_values_sequence ignores stop token but not bound, slice manually
            vals = vals[: max(0, min(len(vals), bound - (pos + 1)))]
            # Keep only the first LABELS_TOP values; extra lines will be joined in zipper
            result["top"][b] = vals

    # Collect BOTTOM values sequence if anchor exists
    if bot_anchor is not None:
        # Gather all value lines after bot_anchor until next section (history) or end
        after = []
        for ln in block_lines[bot_anchor + 1 :]:
            t = _norm_token(ln)
            if t in { _norm_token("Two-Year Payment History"), _norm_token("Days Late - 7 Year History") }:
                break
            if _is_noise_line(ln):
                continue
            # skip pure bureau tokens in this region; many OCRs omit them here
            if t in tokens:
                continue
            after.append(ln.strip())
        # If the total count fits 3x of labels, split equally
        parts = _split_equal_parts(after, 3)
        if parts:
            for idx, b in enumerate(order):
                result["bottom"][b] = parts[idx]
        else:
            # Best effort: assign sequentially by chunk of LABELS_BOTTOM
            for idx, b in enumerate(order):
                start = idx * len(LABELS_BOTTOM)
                result["bottom"][b] = after[start : start + len(LABELS_BOTTOM)]

    return result


# ------------------------------ G2: TOP-only parser ------------------------------
def _g2_make_label_pattern(label: str) -> re.Pattern[str]:
    esc = (
        label.replace("-", "[-–]")
        .replace("#", r"\s*#")
        .replace(" ", r"\s+")
    )
    return re.compile(rf"^{esc}\s*:?$", re.IGNORECASE)


_G2_TOP_PATS: list[tuple[re.Pattern[str], str]] = [
    (_g2_make_label_pattern(l), l) for l in LABELS_TOP
]


def _g2_is_bureau(s: str) -> str | None:
    t = _norm_hdr(s)
    if "TRANSUNION" in t:
        return "transunion"
    if "EXPERIAN" in t:
        return "experian"
    if "EQUIFAX" in t:
        return "equifax"
    return None


def _g2_find_top_group(lines: list[str]) -> tuple[int | None, int | None, list[str]]:
    """Return (start, end, labels) for the first TOP label group with >=3 labels."""
    n = len(lines)
    for i in range(n):
        j = i
        found: list[str] = []
        while j < n:
            m = None
            for pat, lab in _G2_TOP_PATS:
                if pat.match(lines[j].strip()):
                    m = lab
                    break
            if not m:
                break
            found.append(m)
            j += 1
        if len(found) >= 3:
            return i, j - 1, found
    return None, None, []


def _g2_detect_order(lines: list[str], start_idx: int) -> list[str]:
    order: list[str] = []
    for s in lines[start_idx:]:
        b = _g2_is_bureau(s)
        if b and b not in order:
            order.append(b)
            if len(order) == 3:
                break
    return order


def _g2_parse_top_only(lines: list[str]) -> tuple[dict[str, dict[str, Any]] | None, dict[str, int], int]:
    """Parse only the TOP label group into per-bureau fields.

    Returns (fields_by_bureau, counts_by_bureau, joined_fragments_count) or (None, {}, 0) if not matched.
    """
    start, end, labs = _g2_find_top_group(lines)
    if start is None or end is None or not labs:
        return None, {}, 0
    n_labels = len(labs)
    # Determine bureau order by scanning after the group
    order = _g2_detect_order(lines, end + 1)
    if not order:
        return None, {}, 0
    # Stop markers
    STOP_PAT = re.compile(
        r"^(Two\s*[- ]\s*Year\s+Payment\s+History|Days\s+Late|Account\s*#|Account\s+Status|Payment\s+Status)",
        re.IGNORECASE,
    )

    # Collect raw values per bureau
    i = end + 1
    fields: dict[str, dict[str, Any]] = {b: {} for b in order}
    counts: dict[str, int] = {b: 0 for b in order}
    joined_total = 0
    for b in order:
        vals: list[str] = []
        while i < len(lines):
            s = lines[i].strip()
            if _is_noise_line(s):
                i += 1
                continue
            # next bureau or stop
            if _g2_is_bureau(s) and _g2_is_bureau(s) != b:
                break
            if STOP_PAT.match(s):
                break
            # new top label group implies end
            if any(pat.match(s) for pat, _ in _G2_TOP_PATS):
                break
            vals.append(s)
            i += 1
        # Zip values to labels
        mapped = _zip_values_to_labels(vals, labs)
        # Count joined fragments
        if len(vals) > n_labels:
            joined_total += len(vals) - n_labels
        # Map to snake_case keys into fields[b]
        key_map = {
            "Account #": "account_number_display",
            "High Balance": "high_balance",
            "Last Verified": "last_verified",
            "Date of Last Activity": "date_of_last_activity",
            "Date Reported": "date_reported",
            "Date Opened": "date_opened",
            "Balance Owed": "balance_owed",
            "Closed Date": "closed_date",
            "Account Rating": "account_rating",
            "Account Description": "account_description",
            "Dispute Status": "dispute_status",
            "Creditor Type": "creditor_type",
        }
        for lab, val in mapped.items():
            fields[b][key_map[lab]] = val
        counts[b] = sum(1 for v in fields[b].values() if v)
        # move to next bureau token if current line was a bureau header consumed implicitly
        while i < len(lines) and not _g2_is_bureau(lines[i]) and not STOP_PAT.match(lines[i]):
            # Skip residuals until next header; safety hatch
            break
        # if next line is bureau token for next bureau, it will be handled in next loop
    return fields, counts, joined_total


# ------------------------------ G3: BOTTOM-only parser ------------------------------
_G3_BOTTOM_PATS: list[tuple[re.Pattern[str], str]] = [
    (_g2_make_label_pattern(l), l) for l in LABELS_BOTTOM
]


def _g3_find_bottom_group(lines: list[str]) -> tuple[int | None, int | None, list[str]]:
    n = len(lines)
    for i in range(n):
        j = i
        found: list[str] = []
        while j < n:
            m = None
            for pat, lab in _G3_BOTTOM_PATS:
                if pat.match(lines[j].strip()):
                    m = lab
                    break
            if not m:
                break
            found.append(m)
            j += 1
        if len(found) >= 3:  # require at least a few labels for robustness
            return i, j - 1, found
    return None, None, []


def _g3_parse_bottom_only(lines: list[str]) -> tuple[dict[str, dict[str, Any]] | None, dict[str, int], int]:
    start, end, labs = _g3_find_bottom_group(lines)
    if start is None or end is None or not labs:
        return None, {}, 0
    n_labels = len(labs)
    order = _g2_detect_order(lines, end + 1)
    if not order:
        return None, {}, 0
    STOP_PAT = re.compile(
        r"^(Two\s*[- ]\s*Year\s+Payment\s+History|Days\s+Late|Account\s*#|Account\s+Description|Account\s+Rating|Creditor\s+Type)",
        re.IGNORECASE,
    )

    i = end + 1
    fields: dict[str, dict[str, Any]] = {b: {} for b in order}
    counts: dict[str, int] = {b: 0 for b in order}
    joined_total = 0
    for b in order:
        vals: list[str] = []
        while i < len(lines):
            s = lines[i].strip()
            if _is_noise_line(s):
                i += 1
                continue
            if _g2_is_bureau(s) and _g2_is_bureau(s) != b:
                break
            if STOP_PAT.match(s):
                break
            # If a new bottom group starts, stop
            if any(pat.match(s) for pat, _ in _G3_BOTTOM_PATS):
                break
            vals.append(s)
            i += 1
        mapped = _zip_values_to_labels(vals, labs)
        if len(vals) > n_labels:
            joined_total += len(vals) - n_labels
        key_map = {
            "Account Status": "account_status",
            "Payment Status": "payment_status",
            "Creditor Remarks": "creditor_remarks",
            "Payment Amount": "payment_amount",
            "Last Payment": "last_payment",
            "Term Length": "term_length",
            "Past Due Amount": "past_due_amount",
            "Account Type": "account_type",
            "Payment Frequency": "payment_frequency",
            "Credit Limit": "credit_limit",
        }
        for lab, val in mapped.items():
            fields[b][key_map[lab]] = val
        counts[b] = sum(1 for v in fields[b].values() if v)
    return fields, counts, joined_total


# ------------------------------ G4: presence, money cleaning, account tail ------------------------------
BUREAUS = ("transunion", "experian", "equifax")

_MONEY_KEYS = {
    "high_balance",
    "balance_owed",
    "credit_limit",
    "payment_amount",
    "past_due_amount",
}


def _g4_clean_money(val: str | None) -> str:
    if not isinstance(val, str):
        return ""
    s = val.strip()
    if s in {"--", "-", ""}:
        return ""
    # gentle remove of $ and commas
    s = s.replace("$", "").replace(",", "").strip()
    return s


def _g4_apply(fields: dict[str, dict[str, Any]] | None, meta: dict, lines: list[str]) -> tuple[dict[str, dict[str, Any]], dict]:
    fields = fields or {}
    # Clean values and compute presence
    presence: dict[str, bool] = {}
    for b in BUREAUS:
        bureau_map = fields.get(b) or {}
        cleaned: dict[str, Any] = {}
        for k, v in bureau_map.items():
            val = v if isinstance(v, str) else ("" if v is None else str(v))
            if isinstance(val, str) and val.strip() in {"--", "-", ""}:
                val = ""
            if k in _MONEY_KEYS:
                val = _g4_clean_money(val)
            cleaned[k] = val
        fields[b] = cleaned
        presence[b] = any(isinstance(v, str) and v.strip() for v in cleaned.values())

    # Account number tail — if not already present, try to derive from fields
    if not meta.get("account_number_tail"):
        import re as _re
        tail_found = None
        for b in BUREAUS:
            acct = (fields.get(b) or {}).get("account_number_display") or (fields.get(b) or {}).get("account_number")
            if isinstance(acct, str):
                m = _re.search(r"\*+(\d{2,4})\b", acct)
                if m:
                    tail_found = m.group(1)
                    break
        meta["account_number_tail"] = tail_found

    # Update presence
    meta.setdefault("bureau_presence", {})
    meta["bureau_presence"].update(presence)

    if BLOCK_DEBUG:
        try:
            logger.info(
                "ENRICH:G4 presence tu=%s ex=%s eq=%s tail=%s",
                presence.get("transunion", False),
                presence.get("experian", False),
                presence.get("equifax", False),
                meta.get("account_number_tail") or "",
            )
        except Exception:
            pass

    return fields, meta


# ------------------------------ G5: payment history and days-late meta (raw) ------------------------------
def _g5_enrich_meta(meta: dict, lines: list[str]) -> dict:
    try:
        # Two-Year Payment History tokens per bureau
        hist: dict[str, list[str]] = {}
        dl7: dict[str, dict[str, str]] = {}

        # Normalize token function and predicates
        def is_history_hdr(s: str) -> bool:
            t = _norm_token(s)
            return t.startswith(_norm_token("Two-Year Payment History"))

        def is_days_hdr(s: str) -> bool:
            t = _norm_token(s)
            return t.startswith(_norm_token("Days Late - 7 Year History"))

        tokens = {"TRANSUNION", "EXPERIAN", "EQUIFAX"}

        # Payment history
        try:
            start = next((i for i, ln in enumerate(lines) if is_history_hdr(ln)), None)
            if start is not None:
                i = start + 1
                current: str | None = None
                while i < len(lines):
                    t = _norm_token(lines[i])
                    if t in tokens:
                        current = t.lower()
                        hist.setdefault(current, [])
                        i += 1
                        continue
                    # stop if next section header
                    if is_days_hdr(lines[i]) or any(pat.match(lines[i].strip()) for pat, _ in _G2_TOP_PATS + _G3_BOTTOM_PATS):
                        break
                    if current:
                        val = lines[i].strip()
                        if not _is_noise_line(val):
                            hist[current].append(val)
                    i += 1
        except Exception:
            pass

        # Days Late - 7 Year History
        try:
            start2 = next((i for i, ln in enumerate(lines) if is_days_hdr(ln)), None)
            if start2 is not None:
                i = start2 + 1
                current: str | None = None
                import re as _re
                while i < len(lines):
                    t = _norm_token(lines[i])
                    if t in tokens:
                        current = t.lower()
                        dl7.setdefault(current, {"30": "0", "60": "0", "90": "0"})
                        i += 1
                        continue
                    if any(pat.match(lines[i].strip()) for pat, _ in _G2_TOP_PATS + _G3_BOTTOM_PATS) or is_history_hdr(lines[i]):
                        break
                    if current:
                        m30 = _re.search(r"30\s*:\s*(\d+)", lines[i])
                        m60 = _re.search(r"60\s*:\s*(\d+)", lines[i])
                        m90 = _re.search(r"90\s*:\s*(\d+)", lines[i])
                        if m30:
                            dl7[current]["30"] = m30.group(1)
                        if m60:
                            dl7[current]["60"] = m60.group(1)
                        if m90:
                            dl7[current]["90"] = m90.group(1)
                    i += 1
        except Exception:
            pass

        if hist:
            meta.setdefault("payment_history", {})
            meta["payment_history"].update(hist)
        if dl7:
            meta.setdefault("days_late_7y", {})
            meta["days_late_7y"].update(dl7)
    except Exception:
        # best-effort only
        pass
    return meta


# ---------------------------------------------------------------------------
# Canonicalization helpers for issuer headings
# ---------------------------------------------------------------------------

# Patterns apply to a basic-normalized form (lowercased, non-alnum -> single space)
CANON_MAP: dict[str, str] = {
    r"^bk of amer.*$": "Bank of America",
    r"^bankamerica.*$": "Bank of America",
    r"^bofa.*$": "Bank of America",
    r"^jpmcb( card)?$": "JPMorgan Chase",
    r"^chase.*$": "JPMorgan Chase",
    r"^wfbna( card)?$": "Wells Fargo",
    r"^us bk cacs$": "U.S. Bank",
    r"^syncb.*$": "Synchrony Bank",
    r"^cbna$": "Citibank",
    r"^amex$": "American Express",
    r"^nstar cooper$": "NSTAR/COOPER",
    r"^seterus inc$": "Seterus",
    r"^roundpoint$": "RoundPoint",
}

_CANON_PATTERNS: list[tuple[re.Pattern[str], str]] = [
    (re.compile(pat, re.I), canon) for pat, canon in CANON_MAP.items()
]


def _basic_normalize(s: str) -> str:
    s = (s or "").strip().lower()
    # Replace non-alphanumeric with spaces, collapse to single spaces
    s = re.sub(r"[^a-z0-9]+", " ", s)
    return s.strip()


def _slugify(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"[^a-z0-9]+", "-", s)
    s = re.sub(r"-+", "-", s).strip("-")
    return s or "account"


def normalize_heading(raw: str) -> dict:
    """Return canonical issuer fields for a raw heading.

    Returns a dict with keys: canonical, slug, variant.
    """
    variant = (raw or "").strip()
    base = _basic_normalize(variant)
    canonical = None
    for pat, canon in _CANON_PATTERNS:
        if pat.match(base):
            canonical = canon
            break
    if not canonical:
        canonical = base.title()
    slug = _slugify(canonical)
    logger.info(
        "BLOCK: canonical issuer heading=%r -> canonical=%r slug=%r",
        raw,
        canonical,
        slug,
    )
    return {"canonical": canonical, "slug": slug, "variant": variant}


def tail_digits_from_lines(lines: list[str]) -> str | None:
    """Attempt to extract last 2–4 digits of a masked account number.

    Looks for patterns like '****1234', 'XX1234', '1234****' on lines
    containing the word 'Account'/'Acct'. Returns the tail digits or None.
    """
    if not isinstance(lines, list):
        return None
    # Prefer lines that mention account number
    candidates = [
        ln for ln in lines if isinstance(ln, str) and re.search(r"\b(acc(?:ount)?|acct)\b", ln, re.I)
    ] or [ln for ln in lines if isinstance(ln, str)]
    for ln in candidates:
        s = str(ln)
        # masked then digits
        m = re.search(r"(?:[*xX]{2,}|\*{2,}|[xX]{2,})\s*(\d{2,4})\b", s)
        if m:
            return m.group(1)
        # digits then mask
        m = re.search(r"\b(\d{2,4})\s*(?:[*xX]{2,}|\*{2,}|[xX]{2,})", s)
        if m:
            return m.group(1)
        # fallback: trailing 2–4 digits at end of line
        m = re.search(r"(\d{2,4})\s*$", s)
        if m:
            return m.group(1)
    return None


def _split_vals(text: str, parts: int) -> list[str]:
    """Split ``text`` into ``parts`` values using column heuristics."""

    if not text:
        return [""] * parts

    vals = re.split(r"\s{2,}", text.strip())
    if len(vals) != parts:
        tokens = text.strip().split()
        if len(tokens) >= parts:
            vals = tokens[: parts - 1] + [" ".join(tokens[parts - 1 :])]
        else:
            vals = tokens + [""] * (parts - len(tokens))
    if len(vals) > parts:
        vals = vals[: parts - 1] + [" ".join(vals[parts - 1 :])]
    if len(vals) < parts:
        vals += [""] * (parts - len(vals))
    return [v.strip() for v in vals]


def enrich_block_v2(blk: dict) -> dict:
    """Enhanced enrichment for SmartCredit 3-bureau account blocks.

    Parses tri-bureau header order, then maps known labels into per-bureau fields.
    Keeps fields empty and meta consistent when header is missing.
    """

    heading = blk.get("heading", "")
    logger.warning("ENRICH: start heading=%r", heading)

    raw_lines = [str(line or "") for line in (blk.get("lines") or [])]
    lines = [(line or "").strip() for line in raw_lines]
    # G1 debug: label groups + bureaus seen
    if BLOCK_DEBUG and _G1_AVAILABLE:
        try:
            groups = G1_find_label_groups(raw_lines)
            bureaus_seen = sorted({
                b for b in (G1_is_bureau(ln) for ln in raw_lines) if b
            })
            logger.info(
                "ENRICH:G1 label_groups=%s bureaus_seen=%s",
                json.dumps(groups, ensure_ascii=False),
                ",".join(bureaus_seen),
            )
        except Exception:
            pass
    hdr_idx, order = _find_bureau_header_idx(lines)
    if not order:
        det = detect_bureau_order(lines)
        if det:
            order = det
            for i2, ln in enumerate(lines):
                low = ln.lower()
                if all(b in low for b in order):
                    hdr_idx = i2
                    break

    canon = normalize_heading(heading)
    tail = tail_digits_from_lines(lines)
    if not order:
        # No tri-header detected. Proceed to G2/G3 fallbacks below.
        order = []

    # Try stacked-segment parse first (bureau-wise values)
    segments = _segments_by_bureau(raw_lines, order)
    if segments:
        fields_segm: dict[str, dict[str, Any]] = {b: {} for b in order}
        # Map TOP
        for b in order:
            top_vals = segments.get("top", {}).get(b, [])
            if top_vals:
                mapped = _zip_values_to_labels(top_vals, LABELS_TOP)
                # Normalize label keys to snake_case set
                for lab, val in mapped.items():
                    key = {
                        "Account #": "account_number_display",
                        "High Balance": "high_balance",
                        "Last Verified": "last_verified",
                        "Date of Last Activity": "date_of_last_activity",
                        "Date Reported": "date_reported",
                        "Date Opened": "date_opened",
                        "Balance Owed": "balance_owed",
                        "Closed Date": "closed_date",
                        "Account Rating": "account_rating",
                        "Account Description": "account_description",
                        "Dispute Status": "dispute_status",
                        "Creditor Type": "creditor_type",
                    }[lab]
                    fields_segm[b][key] = val
        # Map BOTTOM
        for b in order:
            bot_vals = segments.get("bottom", {}).get(b, [])
            if bot_vals:
                mapped = _zip_values_to_labels(bot_vals, LABELS_BOTTOM)
                for lab, val in mapped.items():
                    key = {
                        "Account Status": "account_status",
                        "Payment Status": "payment_status",
                        "Creditor Remarks": "creditor_remarks",
                        "Payment Amount": "payment_amount",
                        "Last Payment": "last_payment",
                        "Term Length": "term_length",
                        "Past Due Amount": "past_due_amount",
                        "Account Type": "account_type",
                        "Payment Frequency": "payment_frequency",
                        "Credit Limit": "credit_limit",
                    }[lab]
                    fields_segm[b][key] = val

        # Fill presence/meta, apply G4 cleaning/derivations, return
        presence = {
            b: any(v not in (None, "") for v in fields_segm.get(b, {}).values()) for b in ["transunion", "experian", "equifax"]
        }
        base_meta = dict(blk.get("meta") or {})
        meta = {
            "issuer_canonical": canon["canonical"],
            "issuer_slug": canon["slug"],
            "issuer_variant": canon["variant"],
            "bureau_presence": presence,
        }
        if tail:
            meta["account_number_tail"] = tail
        if BLOCK_DEBUG:
            meta.setdefault("debug", {})
            meta["debug"].update({
                "top_counts": {b: len(segments.get("top", {}).get(b, [])) for b in order},
                "bottom_counts": {b: len(segments.get("bottom", {}).get(b, [])) for b in order},
            })
        cleaned_fields, cleaned_meta = _g4_apply(fields_segm, {**base_meta, **meta}, raw_lines)
        cleaned_meta = _g5_enrich_meta(cleaned_meta, raw_lines)
        return {**blk, "fields": cleaned_fields, "meta": cleaned_meta}

    # G2: TOP-only parsing fallback (order-agnostic)
    fields_top, counts_top, joined_total = _g2_parse_top_only(raw_lines)
    if fields_top:
        # Also try BOTTOM and merge
        bottom_fields, bottom_counts, bottom_joined = _g3_parse_bottom_only(raw_lines)
        if bottom_fields:
            for b in ("transunion", "experian", "equifax"):
                src = fields_top.setdefault(b, {})
                upd = bottom_fields.get(b, {})
                for k, v in upd.items():
                    if k == "creditor_remarks" and src.get(k) and v:
                        src[k] = f"{src[k]} {v}".strip()
                    else:
                        if v:
                            src[k] = v
        presence = {
            b: any(v not in (None, "") for v in fields_top.get(b, {}).values()) for b in ["transunion", "experian", "equifax"]
        }
        base_meta = dict(blk.get("meta") or {})
        meta = {
            "issuer_canonical": canon["canonical"],
            "issuer_slug": canon["slug"],
            "issuer_variant": canon["variant"],
            "bureau_presence": presence,
        }
        if tail:
            meta["account_number_tail"] = tail
        if BLOCK_DEBUG:
            logger.info(
                "ENRICH:G2 top_counts tu=%d ex=%d eq=%d joined=%d",
                counts_top.get("transunion", 0),
                counts_top.get("experian", 0),
                counts_top.get("equifax", 0),
                joined_total,
            )
            if bottom_fields:
                logger.info(
                    "ENRICH:G3 bottom_counts tu=%d ex=%d eq=%d merged=%s",
                    bottom_counts.get("transunion", 0),
                    bottom_counts.get("experian", 0),
                    bottom_counts.get("equifax", 0),
                    "True",
                )
        cleaned_fields2, cleaned_meta2 = _g4_apply(fields_top, {**base_meta, **meta}, raw_lines)
        cleaned_meta2 = _g5_enrich_meta(cleaned_meta2, raw_lines)
        return {**blk, "fields": cleaned_fields2, "meta": cleaned_meta2}

    # Full key set
    field_keys = [
        "account_number_display",
        "high_balance",
        "last_verified",
        "date_of_last_activity",
        "date_reported",
        "date_opened",
        "balance_owed",
        "closed_date",
        "account_rating",
        "account_description",
        "dispute_status",
        "creditor_type",
        "account_status",
        "payment_status",
        "creditor_remarks",
        "payment_amount",
        "last_payment",
        "term_length",
        "past_due_amount",
        "account_type",
        "payment_frequency",
        "credit_limit",
        "two_year_payment_history",
        "seven_year_days_late",
    ]
    fields: dict[str, dict[str, Any]] = {
        b: {k: None for k in field_keys} for b in ["transunion", "experian", "equifax"]
    }

    start_row = (hdr_idx or 0) + 1
    if hdr_idx is None:
        start_row = 0

    def _split_three(s: str) -> list[str]:
        parts = re.split(r"\t+", s.strip()) if "\t" in s else re.split(r"\s{2,}", s.strip())
        if len(parts) < 3:
            parts += [""] * (3 - len(parts))
        if len(parts) > 3:
            parts = parts[:2] + [" ".join(parts[2:])]
        return [p.strip() for p in parts]

    def _clean_cell(v: str) -> str:
        v = (v or "").strip()
        return "" if v in {"--", "-"} else v

    if hdr_idx is not None:
        logger.info(
            "ENRICH: bureau_header idx=%d order=%s",
            hdr_idx,
            "/".join([o[:2].upper() for o in order]),
        )

    i = start_row
    while i < len(lines):
        row = raw_lines[i].strip()
        if not row:
            i += 1
            continue
        matched = False
        for pat, key in LABEL_MAP:
            m = pat.match(row)
            if not m:
                continue
            matched = True
            rest = row[m.end():].strip()
            rest = rest[1:].strip() if rest.startswith(":") else rest
            if key == "two_year_payment_history":
                block_lines = [row]
                j = i + 1
                while j < len(lines):
                    nxt = raw_lines[j].strip()
                    if not nxt:
                        break
                    if any(p.match(nxt) for p, _ in LABEL_MAP):
                        break
                    block_lines.append(nxt)
                    j += 1
                blob = "\n".join(block_lines)
                for b in order:
                    fields[b]["two_year_payment_history"] = blob
                i = j
                break
            if key == "seven_year_days_late":
                candidate_lines = [rest] if rest else []
                look = 3
                j = i + 1
                while j < len(lines) and look > 0:
                    candidate_lines.append(raw_lines[j].strip())
                    look -= 1
                    j += 1
                selected = None
                for s in candidate_lines:
                    if not s:
                        continue
                    if re.search(r"\t|\s{2,}", s):
                        selected = s
                        break
                selected = selected or (candidate_lines[0] if candidate_lines else "")
                cols = _split_three(selected)
                for idx2, b in enumerate(order):
                    col = cols[idx2] if idx2 < len(cols) else ""
                    m30 = re.search(r"30\s*:\s*(\d+)", col)
                    m60 = re.search(r"60\s*:\s*(\d+)", col)
                    m90 = re.search(r"90\s*:\s*(\d+)", col)
                    if m30 or m60 or m90:
                        fields[b]["seven_year_days_late"] = {
                            "30": int(m30.group(1)) if m30 else 0,
                            "60": int(m60.group(1)) if m60 else 0,
                            "90": int(m90.group(1)) if m90 else 0,
                        }
                i += 1
                break
            cols = _split_three(rest)
            for idx2, b in enumerate(order):
                v = _clean_cell(cols[idx2] if idx2 < len(cols) else "")
                fields[b][key] = v
            if BLOCK_DEBUG:
                logger.info(
                    "ENRICH: parsed %s -> tu=%r ex=%r eq=%r",
                    key,
                    fields[order[0]].get(key),
                    fields[order[1]].get(key),
                    fields[order[2]].get(key),
                )
            break
        if not matched:
            i += 1
            continue

    tu_count = sum(1 for v in fields["transunion"].values() if v not in (None, ""))
    ex_count = sum(1 for v in fields["experian"].values() if v not in (None, ""))
    eq_count = sum(1 for v in fields["equifax"].values() if v not in (None, ""))
    if BLOCK_DEBUG:
        logger.warning(
            "ENRICH: fields_done tu=%d ex=%d eq=%d", tu_count, ex_count, eq_count
        )
        logger.info(
            "BLOCK: enrichment_summary heading=%r tu_filled=%d ex_filled=%d eq_filled=%d",
            heading,
            tu_count,
            ex_count,
            eq_count,
        )

    presence = {
        "transunion": any(v not in (None, "") for v in fields.get("transunion", {}).values()),
        "experian": any(v not in (None, "") for v in fields.get("experian", {}).values()),
        "equifax": any(v not in (None, "") for v in fields.get("equifax", {}).values()),
    }
    if BLOCK_DEBUG:
        logger.info(
            "BLOCK: bureau_presence tu=%d ex=%d eq=%d tail=%s",
            1 if presence["transunion"] else 0,
            1 if presence["experian"] else 0,
            1 if presence["equifax"] else 0,
            tail or "",
        )
    base_meta = dict(blk.get("meta") or {})
    meta = {
        "issuer_canonical": canon["canonical"],
        "issuer_slug": canon["slug"],
        "issuer_variant": canon["variant"],
        "bureau_presence": presence,
    }
    if tail:
        meta["account_number_tail"] = tail
    if BLOCK_DEBUG:
        meta.setdefault("debug", {})
        meta["debug"].update({"bureau_header_line": (hdr_idx + 1) if hdr_idx is not None else None, "head_sample": raw_lines[:8]})
    cleaned_fields3, cleaned_meta3 = _g4_apply(fields, {**base_meta, **meta}, raw_lines)
    cleaned_meta3 = _g5_enrich_meta(cleaned_meta3, raw_lines)
    return {**blk, "fields": cleaned_fields3, "meta": cleaned_meta3}
def enrich_block(blk: dict) -> dict:
    """Add structured ``fields`` map parsed from ``blk['lines']``."""

    heading = blk.get("heading", "")
    logger.warning("ENRICH: start heading=%r", heading)

    # Normalize anomalies in potential bureau header lines
    lines = [
        (line or "").replace("®", "").strip() for line in (blk.get("lines") or [])
    ]
    order = detect_bureau_order(lines)
    # Always compute canonical issuer + tail/meta, even if no bureau order found
    canon = normalize_heading(heading)
    tail = tail_digits_from_lines(lines)

    if not order:
        logger.warning(
            "ENRICH: no bureau columns; skipping enrichment heading=%r", heading
        )
        base_meta = dict(blk.get("meta") or {})
        meta = {
            "issuer_canonical": canon["canonical"],
            "issuer_slug": canon["slug"],
            "issuer_variant": canon["variant"],
            "bureau_presence": {
                "transunion": False,
                "experian": False,
                "equifax": False,
            },
        }
        if tail:
            meta["account_number_tail"] = tail
        meta = {**base_meta, **meta}
        return {**blk, "fields": {}, "meta": meta}

    # initialise fields map with empty strings
    field_keys = list(FIELD_LABELS.values())
    fields = {
        b: {k: "" for k in field_keys} for b in ["transunion", "experian", "equifax"]
    }

    in_section = False
    for line in blk.get("lines") or []:
        clean = line.strip()
        if not clean:
            continue
        if not in_section:
            norm = re.sub(r"[^a-z]+", " ", clean.lower())
            if all(b in norm for b in order):
                in_section = True
            continue

        norm_line = clean.lower()
        for label, key in FIELD_LABELS.items():
            if norm_line.startswith(label):
                rest = clean[len(label) :].strip()
                if rest.startswith(":"):
                    rest = rest[1:].strip()
                vals = _split_vals(rest, len(order))
                for idx, bureau in enumerate(order):
                    v = vals[idx] if idx < len(vals) else ""
                    v = v if v not in {"--", "-"} else ""
                    fields[bureau][key] = v
                break

    tu_count = sum(1 for v in fields["transunion"].values() if v)
    ex_count = sum(1 for v in fields["experian"].values() if v)
    eq_count = sum(1 for v in fields["equifax"].values() if v)
    if BLOCK_DEBUG:
        logger.warning(
            "ENRICH: fields_done tu=%d ex=%d eq=%d", tu_count, ex_count, eq_count
        )
        logger.info(
            "BLOCK: enrichment_summary heading=%r tu_filled=%d ex_filled=%d eq_filled=%d",
            heading,
            tu_count,
            ex_count,
            eq_count,
        )

    # Meta: presence flags and identifiers
    presence = {
        "transunion": any(bool(v) for v in fields.get("transunion", {}).values()),
        "experian": any(bool(v) for v in fields.get("experian", {}).values()),
        "equifax": any(bool(v) for v in fields.get("equifax", {}).values()),
    }
    logger.info(
        "BLOCK: bureau_presence tu=%d ex=%d eq=%d tail=%s",
        1 if presence["transunion"] else 0,
        1 if presence["experian"] else 0,
        1 if presence["equifax"] else 0,
        tail or "",
    )
    base_meta = dict(blk.get("meta") or {})
    meta = {
        "issuer_canonical": canon["canonical"],
        "issuer_slug": canon["slug"],
        "issuer_variant": canon["variant"],
        "bureau_presence": presence,
    }
    if tail:
        meta["account_number_tail"] = tail
    meta = {**base_meta, **meta}

    return {**blk, "fields": fields, "meta": meta}


def export_account_blocks(
    session_id: str, pdf_path: str | Path
) -> List[Dict[str, Any]]:
    """Extract account blocks from ``pdf_path`` and export them to JSON files.

    Parameters
    ----------
    session_id:
        Identifier used for the output directory ``traces/blocks/<session_id>``.
    pdf_path:
        Path to the PDF to parse.

    Returns
    -------
    list[dict]
        The list of account block dictionaries, each containing ``heading`` and
        ``lines`` keys.
    """
    cached = load_cached_text(session_id)
    if not cached:
        raise ValueError("no_cached_text_for_session")
    text = cached["full_text"]

    # TP3: Preload layout tokens per page (best-effort, soft-fail)
    layout_pages: list[dict] = []
    TRY_LAYOUT = USE_LAYOUT_TEXT
    if TRY_LAYOUT:
        try:
            layout = load_text_with_layout(str(pdf_path))
            layout_pages = list(layout.get("pages") or [])
        except Exception:
            layout_pages = []
    logger.info("BLOCK: segmentation start sid=%s", session_id)
    fbk_blocks: List[Dict[str, Any]] = segment_account_blocks(text)

    if not fbk_blocks:
        logger.error(
            "BLOCKS_FAIL_FAST: 0 blocks extracted sid=%s file=%s",
            session_id,
            str(pdf_path),
        )
        raise ValueError("No blocks extracted")

    blocks_by_account_fuzzy = build_block_fuzzy(fbk_blocks) if fbk_blocks else {}
    logger.warning(
        "ANZ: pre-save fbk=%d fuzzy=%d sid=%s",
        len(fbk_blocks),
        len(blocks_by_account_fuzzy or {}),
        session_id,
    )

    out_dir = Path("traces") / "blocks" / session_id
    out_dir.mkdir(parents=True, exist_ok=True)
    logger.warning("BLOCK_ENRICH: enabled=%s sid=%s", ENRICH_ENABLED, session_id)

    # TP4: Optional debug dump of layout tokens per page
    if BLOCK_DEBUG and layout_pages:
        for idx, pg in enumerate(layout_pages, start=1):
            try:
                dbg_path = out_dir / f"_debug_layout_page{idx:03d}.tsv.json"
                with dbg_path.open("w", encoding="utf-8") as f:
                    json.dump({"number": pg.get("number", idx), "width": pg.get("width"), "height": pg.get("height"), "tokens": pg.get("tokens", [])}, f, ensure_ascii=False)
            except Exception:
                pass

    out_blocks: List[Dict[str, Any]] = []
    # Track canonical -> variants mapping to summarize at the end
    canon_variants: dict[str, set[str]] = {}
    idx_info = []
    for i, blk in enumerate(fbk_blocks, 1):
        # Log basic block info before enrichment
        btype = (blk.get("meta") or {}).get("block_type") if isinstance(blk, dict) else None
        logger.info(
            "BLOCK: new block i=%d type=%s heading=%r lines=%d",
            i,
            btype or "unknown",
            blk.get("heading") if isinstance(blk, dict) else "",
            len(blk.get("lines") or []) if isinstance(blk, dict) else 0,
        )
        out_blk = enrich_block_v2(blk) if ENRICH_ENABLED else blk

        # TP3: Column-reader integration to ensure full field sets per bureau
        try:
            # Map block line range to page indices using cached pages line counts
            page_texts = list((cached.get("pages") or []))
            starts: list[int] = []  # 1-based line start per page
            ends: list[int] = []    # 1-based line end per page (inclusive)
            acc = 1
            for ptxt in page_texts:
                n_lines = len(str(ptxt or "").splitlines())
                if n_lines <= 0:
                    n_lines = 0
                starts.append(acc)
                ends.append(acc + max(0, n_lines) - 1)
                acc += max(0, n_lines)

            meta = out_blk.get("meta") or {}
            sline = int(meta.get("start_line", 1))
            eline = int(meta.get("end_line", sline))
            # Determine overlapping pages
            page_idxs: list[int] = []  # 0-based index into cached pages / layout_pages
            for idx, (ps, pe) in enumerate(zip(starts, ends)):
                if ps == 0 and pe == 0:
                    continue
                # Overlap [sline, eline) with [ps, pe]
                if sline <= pe and (eline - 1) >= ps:
                    page_idxs.append(idx)

            # Gather tokens for these pages
            page_tokens: list[dict] = []
            for idx in page_idxs:
                if idx < len(layout_pages):
                    toks = layout_pages[idx].get("tokens") or []
                    page_tokens.extend(toks)

            # Extract layout-based fields and merge with existing enriched fields
            fields_map = extract_bureau_table({
                "layout_tokens": page_tokens,
                "meta": {"debug": {}}
            })
            base_fields = dict(out_blk.get("fields") or {})
            merged: dict[str, dict[str, Any]] = {}
            all_keys = list(FIELD_LABELS.values())
            for bureau in ("transunion", "experian", "equifax"):
                dst = {k: base_fields.get(bureau, {}).get(k, "") for k in all_keys}
                src = fields_map.get(bureau, {})
                for k in all_keys:
                    v = (src.get(k) or "").strip()
                    if v:
                        if k == "creditor_remarks" and dst.get(k):
                            dst[k] = f"{dst[k]} {v}".strip()
                        else:
                            dst[k] = v
                merged[bureau] = dst

            # Apply cleaning and presence derivations
            cleaned_fields, cleaned_meta = _g4_apply(
                merged, dict(out_blk.get("meta") or {}), out_blk.get("lines") or []
            )

            # TP4: augment meta.debug with bureau header line and sample (first 5 lines after heading)
            if BLOCK_DEBUG:
                try:
                    hdr_idx, _order = _find_bureau_header_idx([str(x or "") for x in (out_blk.get("lines") or [])])
                except Exception:
                    hdr_idx = None
                cleaned_meta.setdefault("debug", {})
                cleaned_meta["debug"]["bureau_header_line"] = (hdr_idx + 1) if hdr_idx is not None else None
                sample = []
                try:
                    ls = out_blk.get("lines") or []
                    sample = [str(x) for x in ls[1:6]]
                except Exception:
                    sample = []
                cleaned_meta["debug"]["sample"] = sample

            # Log deterministic field presence (counts of keys)
            if BLOCK_DEBUG:
                tu_n = len(cleaned_fields.get("transunion", {}))
                ex_n = len(cleaned_fields.get("experian", {}))
                eq_n = len(cleaned_fields.get("equifax", {}))
                missing = 0
                try:
                    logger.warning(
                        "ENRICH: fields_done tu=%d ex=%d eq=%d missing=%d",
                        tu_n,
                        ex_n,
                        eq_n,
                        missing,
                    )
                except Exception:
                    pass

            out_blk = {**out_blk, "fields": cleaned_fields, "meta": cleaned_meta}
        except Exception:
            # Keep out_blk as generated by original enrich path when column-reader fails
            pass
        out_blocks.append(out_blk)
        jpath = out_dir / f"block_{i:02d}.json"
        with jpath.open("w", encoding="utf-8") as f:
            json.dump(out_blk, f, ensure_ascii=False, indent=2)
        idx_info.append({"i": i, "heading": out_blk["heading"], "file": str(jpath)})
        # Accumulate canonical variant counts
        try:
            meta = out_blk.get("meta", {})
            canon = meta.get("issuer_canonical")
            variant = meta.get("issuer_variant")
            if canon and isinstance(variant, str):
                canon_variants.setdefault(canon, set()).add(variant)
        except Exception:
            pass

    with (out_dir / "_index.json").open("w", encoding="utf-8") as f:
        json.dump(idx_info, f, ensure_ascii=False, indent=2)

    # Log canonicalization summary + segmentation summary
    try:
        summary = {k: len(v) for k, v in sorted(canon_variants.items(), key=lambda x: x[0])}
        logger.debug("BLOCK: canonicalization_summary %s", json.dumps(summary, ensure_ascii=False, sort_keys=True))
    except Exception:
        pass

    try:
        accounts = sum(1 for b in out_blocks if (b.get("meta") or {}).get("block_type") == "account")
        summaries = sum(1 for b in out_blocks if (b.get("meta") or {}).get("block_type") == "summary")
        logger.info(
            "BLOCK: segmentation summary accounts=%d summaries=%d total=%d",
            accounts,
            summaries,
            len(out_blocks),
        )
    except Exception:
        pass

    logger.warning(
        "ANZ: export blocks sid=%s dir=%s files=%d",
        session_id,
        str(out_dir),
        len(out_blocks),
    )

    return out_blocks

# ----------------------------------------------------------------------------
# H1 override: enrich_block with TOP-group alignment and continuation joining
# ----------------------------------------------------------------------------
def enrich_block(blk: dict) -> dict:
    """Add structured ``fields`` map parsed from ``blk['lines']``.

    Implements H1: Align TOP-group (N=12) values per bureau with continuation-line
    joining and strict stop conditions. Falls back to the simple column-split
    parser when a TOP label group (>=3 labels) isn't detected.
    """

    heading = blk.get("heading", "")
    if BLOCK_DEBUG:
        logger.warning("ENRICH: start heading=%r", heading)

    # Normalize anomalies in potential bureau header lines
    raw_lines = [str(line or "") for line in (blk.get("lines") or [])]
    lines = [(line or "").replace("Â®", "").strip() for line in raw_lines]

    # Local H1 helpers (scoped to this function)
    TOP_LABELS = [
        "Account #",
        "High Balance",
        "Last Verified",
        "Date of Last Activity",
        "Date Reported",
        "Date Opened",
        "Balance Owed",
        "Closed Date",
        "Account Rating",
        "Account Description",
        "Dispute Status",
        "Creditor Type",
    ]
    BOTTOM_LABELS = [
        "Account Status",
        "Payment Status",
        "Creditor Remarks",
        "Payment Amount",
        "Last Payment",
        "Term Length",
        "Past Due Amount",
        "Account Type",
        "Payment Frequency",
        "Credit Limit",
    ]

    def _norm(s: str) -> str:
        s = (s or "").replace("\u00A0", " ")
        s = re.sub(r"\s+", " ", s.strip())
        s = s.rstrip(":")
        s = re.sub(r"[^A-Za-z0-9 #]+", " ", s)  # keep letters/numbers/space/#
        return s.upper()

    def _is_bureau(s: str) -> str | None:
        t = _norm(s)
        if t == "TRANSUNION":
            return "transunion"
        if t == "EXPERIAN":
            return "experian"
        if t == "EQUIFAX":
            return "equifax"
        return None

    def _is_label(s: str) -> tuple[str | None, str | None]:
        t = _norm(s)
        for lab in TOP_LABELS:
            if t == _norm(lab):
                return "top", lab
        for lab in BOTTOM_LABELS:
            if t == _norm(lab):
                return "bottom", lab
        return None, None

    # 1) Find TOP label group (>=3 consecutive TOP labels)
    top_start_idx: int | None = None
    top_labels: list[str] = []
    i = 0
    while i < len(lines):
        k, lab = _is_label(lines[i])
        if k == "top":
            j = i
            acc: list[str] = []
            while j < len(lines):
                k2, lab2 = _is_label(lines[j])
                if k2 == "top" and lab2:
                    acc.append(lab2)
                    j += 1
                else:
                    break
            if len(acc) >= 3:
                top_start_idx = i
                top_labels = acc
                break
            i = j
            continue
        i += 1

    # Determine bureau order (prefer explicit single-token lines)
    order: list[str] = []
    bureau_idx: dict[str, int] = {}
    if top_start_idx is not None:
        scan_start = max(0, top_start_idx - 2)
        for idx in range(scan_start, len(lines)):
            b = _is_bureau(lines[idx])
            if b and b not in bureau_idx:
                bureau_idx[b] = idx
        order = [b for b, _ in sorted(bureau_idx.items(), key=lambda x: x[1])]
    if not order:
        try:
            order = detect_bureau_order(lines) or []
        except Exception:
            order = []

    # Initialise fields map with existing content preserved
    existing_fields = blk.get("fields") or {}
    field_keys = list(FIELD_LABELS.values())
    fields: dict[str, dict[str, str]] = {
        b: {k: existing_fields.get(b, {}).get(k, "") for k in field_keys}
        for b in ("transunion", "experian", "equifax")
    }

    # TOP label -> snake_case mapping
    top_key_map = {
        "Account #": "account_number_display",
        "High Balance": "high_balance",
        "Last Verified": "last_verified",
        "Date of Last Activity": "date_of_last_activity",
        "Date Reported": "date_reported",
        "Date Opened": "date_opened",
        "Balance Owed": "balance_owed",
        "Closed Date": "closed_date",
        "Account Rating": "account_rating",
        "Account Description": "account_description",
        "Dispute Status": "dispute_status",
        "Creditor Type": "creditor_type",
    }

    CONTINUATION_SINGLETONS = {":", "mortgage", "balance", "account", "finance"}

    def _should_join(prev_val: str, current_line: str, next_line: str | None) -> bool:
        s = (current_line or "").strip()
        if not s:
            return False
        low = s.lower()
        if low in CONTINUATION_SINGLETONS:
            return True
        if low == s and len(low) <= 16 and len(low.split()) <= 2:
            if re.match(r"^[0-9$]", low):
                return False
            return True
        if prev_val and re.search(r"[-/]\s*$", prev_val):
            return True
        if next_line is not None and next_line.strip() == ":":
            return True
        return False

    STOP_SET = {_norm("TWO-YEAR PAYMENT HISTORY"), _norm("DAYS LATE - 7 YEAR HISTORY")}

    def _is_stop_token(s: str) -> bool:
        if _is_bureau(s):
            return True
        kind, _ = _is_label(s)
        if kind is not None:
            return True
        t = _norm(s)
        if t in STOP_SET:
            return True
        return False

    h1_applied = False
    if top_start_idx is not None and order and bureau_idx:
        n_needed = len(top_labels)
        for b in order:
            if b not in bureau_idx:
                continue
            start_i = bureau_idx[b] + 1
            vals: list[str] = []
            idx2 = start_i
            while idx2 < len(lines) and len(vals) < n_needed:
                t = lines[idx2].strip()
                if _is_stop_token(t):
                    break
                if t == "":
                    vals.append("")
                    idx2 += 1
                    continue
                if not vals:
                    vals.append(t)
                else:
                    nxt = lines[idx2 + 1] if (idx2 + 1) < len(lines) else None
                    if _should_join(vals[-1], t, nxt):
                        vals[-1] = (vals[-1] + " " + t).strip() if vals[-1] else t
                    else:
                        vals.append(t)
                idx2 += 1
            while len(vals) < n_needed:
                vals.append("")
            for lab, val in zip(top_labels, vals[:n_needed]):
                key = top_key_map.get(lab)
                if not key:
                    continue
                v = val.strip()
                if v in {"--", "-"}:
                    v = ""
                fields[b][key] = v
        h1_applied = any(
            any(fields[b].get(top_key_map[lab], "") for lab in top_labels if lab in top_key_map)
            for b in order
        )

    # ---------------- H2: BOTTOM group alignment (N=10) per bureau ----------------
    # Find BOTTOM label group (>=3 consecutive labels)
    bottom_start_idx: int | None = None
    bottom_labels: list[str] = []
    i2 = 0
    while i2 < len(lines):
        k, lab = _is_label(lines[i2])
        if k == "bottom":
            j2 = i2
            acc2: list[str] = []
            while j2 < len(lines):
                k3, lab3 = _is_label(lines[j2])
                if k3 == "bottom" and lab3:
                    acc2.append(lab3)
                    j2 += 1
                else:
                    break
            if len(acc2) >= 3:
                bottom_start_idx = i2
                bottom_labels = acc2
                break
            i2 = j2
            continue
        i2 += 1

    # Determine bureau order for BOTTOM (reuse TOP order if available)
    order_bottom: list[str] = order[:] if order else []
    bureau_idx_bottom: dict[str, int] = {}
    if bottom_start_idx is not None:
        scan_from = max(0, bottom_start_idx - 2)
        for idxb in range(scan_from, len(lines)):
            b2 = _is_bureau(lines[idxb])
            if b2 and b2 not in bureau_idx_bottom:
                bureau_idx_bottom[b2] = idxb
        if not order_bottom:
            order_bottom = [b for b, _ in sorted(bureau_idx_bottom.items(), key=lambda x: x[1])]
        if not order_bottom:
            try:
                order_bottom = detect_bureau_order(lines) or []
            except Exception:
                order_bottom = []

    # BOTTOM label -> snake_case mapping
    bottom_key_map = {
        "Account Status": "account_status",
        "Payment Status": "payment_status",
        "Creditor Remarks": "creditor_remarks",
        "Payment Amount": "payment_amount",
        "Last Payment": "last_payment",
        "Term Length": "term_length",
        "Past Due Amount": "past_due_amount",
        "Account Type": "account_type",
        "Payment Frequency": "payment_frequency",
        "Credit Limit": "credit_limit",
    }

    if bottom_start_idx is not None and (order_bottom or bureau_idx_bottom):
        n_needed_bottom = len(bottom_labels)
        for b in (order_bottom or list(bureau_idx_bottom.keys())):
            if b not in bureau_idx_bottom:
                # No explicit bureau token near bottom; skip to avoid spillover
                continue
            start_b = bureau_idx_bottom[b] + 1
            vals_b: list[str] = []
            p = start_b
            while p < len(lines) and len(vals_b) < n_needed_bottom:
                tline = lines[p].strip()
                if _is_stop_token(tline):
                    break
                if tline == "":
                    vals_b.append("")
                    p += 1
                    continue
                if not vals_b:
                    vals_b.append(tline)
                else:
                    nxt2 = lines[p + 1] if (p + 1) < len(lines) else None
                    curr_label = bottom_labels[len(vals_b) - 1] if (len(vals_b) - 1) < len(bottom_labels) else None
                    # For BOTTOM section, only allow joining for Creditor Remarks;
                    # avoid accidental concatenation for status/type fields.
                    join = _should_join(vals_b[-1], tline, nxt2) if curr_label in {"Creditor Remarks"} else False
                    if join:
                        vals_b[-1] = (vals_b[-1] + " " + tline).strip() if vals_b[-1] else tline
                    else:
                        vals_b.append(tline)
                p += 1
            while len(vals_b) < n_needed_bottom:
                vals_b.append("")
            # Merge into fields[b]
            dst = fields.setdefault(b, {})
            for lab, val in zip(bottom_labels, vals_b[:n_needed_bottom]):
                keyb = bottom_key_map.get(lab)
                if not keyb:
                    continue
                v2 = (val or "").strip()
                if v2 in {"--", "-"}:
                    v2 = ""
                if keyb == "creditor_remarks" and dst.get(keyb) and v2:
                    dst[keyb] = f"{dst[keyb]} {v2}".strip()
                elif (not dst.get(keyb)) and v2:
                    dst[keyb] = v2

    # Fallback: simple column-split parsing when H1 didn't execute
    if not h1_applied:
        order2: list[str] = []
        try:
            order2 = detect_bureau_order(lines) or []
        except Exception:
            order2 = []
        if order2:
            in_section = False
            for line in lines:
                clean = line.strip()
                if not clean:
                    continue
                if not in_section:
                    norm = re.sub(r"[^a-z]+", " ", clean.lower())
                    if all(b in norm for b in order2):
                        in_section = True
                    continue
                norm_line = clean.lower()
                for label, key in FIELD_LABELS.items():
                    if norm_line.startswith(label):
                        rest = clean[len(label) :].strip()
                        if rest.startswith(":"):
                            rest = rest[1:].strip()
                        vals = _split_vals(rest, len(order2))
                        for idx3, bureau in enumerate(order2):
                            v = vals[idx3] if idx3 < len(vals) else ""
                            v = v if v not in {"--", "-"} else ""
                            dst2 = fields.setdefault(bureau, {})
                            if key == "creditor_remarks" and dst2.get(key) and v:
                                dst2[key] = f"{dst2[key]} {v}".strip()
                            elif (not dst2.get(key)) and v:
                                dst2[key] = v
                        break

    # H3: Ensure all 22 keys exist per bureau, clean money fields, and set meta
    ALL_KEYS = list(FIELD_LABELS.values())
    for b in ("transunion", "experian", "equifax"):
        dst = fields.setdefault(b, {})
        for k in ALL_KEYS:
            dst.setdefault(k, "")

    # Apply standard cleaning + presence + account tail extraction
    base_meta = dict(blk.get("meta") or {})
    cleaned_fields, cleaned_meta = _g4_apply(fields, base_meta, raw_lines)

    return {**blk, "fields": cleaned_fields, "meta": cleaned_meta}
